{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8d2651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T00:54:39.063396Z",
     "iopub.status.busy": "2024-01-25T00:54:39.063009Z",
     "iopub.status.idle": "2024-01-25T00:55:05.257260Z",
     "shell.execute_reply": "2024-01-25T00:55:05.256161Z"
    },
    "papermill": {
     "duration": 26.202619,
     "end_time": "2024-01-25T00:55:05.259835",
     "exception": false,
     "start_time": "2024-01-25T00:54:39.057216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\r\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/3f/61/047b353f0ad550226ef962da182b4a09b689eb6df6bd84a03e44f9ee95bb/scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\r\n",
      "Collecting numpy<2.0,>=1.19.5 (from scikit-learn)\r\n",
      "  Obtaining dependency information for numpy<2.0,>=1.19.5 from https://files.pythonhosted.org/packages/a5/37/d1453c9ff4f7630e68ec036c6fb56ba0d7c769daa8a4083cb4ef8ee45995/numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Downloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting scipy>=1.6.0 (from scikit-learn)\r\n",
      "  Obtaining dependency information for scipy>=1.6.0 from https://files.pythonhosted.org/packages/f5/aa/8e6071a5e4dca4ec68b5b22e4991ee74c59c5d372112b9c236ec1faff57d/scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\r\n",
      "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\r\n",
      "  Obtaining dependency information for joblib>=1.2.0 from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\r\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\r\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\r\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\r\n",
      "Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\r\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.3 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\r\n",
      "cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\r\n",
      "cuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "numba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.26.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.12.0 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\r\n",
      "raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.3 which is incompatible.\r\n",
      "tensorflowjs 4.15.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\r\n",
      "woodwork 0.27.0 requires numpy<1.25.0,>=1.22.0, but you have numpy 1.26.3 which is incompatible.\r\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.26.3 which is incompatible.\r\n",
      "ydata-profiling 4.5.1 requires scipy<1.12,>=1.4.1, but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed joblib-1.3.2 numpy-1.26.3 scikit-learn-1.4.0 scipy-1.12.0 threadpoolctl-3.2.0\r\n"
     ]
    }
   ],
   "source": [
    "# Install Scikit Learning Lib\n",
    "!pip install -U scikit-learn --target=/kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2c1781c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T00:55:05.275056Z",
     "iopub.status.busy": "2024-01-25T00:55:05.274732Z",
     "iopub.status.idle": "2024-01-25T00:55:09.191941Z",
     "shell.execute_reply": "2024-01-25T00:55:09.190735Z"
    },
    "papermill": {
     "duration": 3.927288,
     "end_time": "2024-01-25T00:55:09.194121",
     "exception": false,
     "start_time": "2024-01-25T00:55:05.266833",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import Pytorch and verify that we have GPU \n",
    "import torch\n",
    "\n",
    "# Make use of a GPU or MPS (Apple) if one is available.\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"mps\" if has_mps else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fc9aa50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T00:55:09.209580Z",
     "iopub.status.busy": "2024-01-25T00:55:09.209181Z",
     "iopub.status.idle": "2024-01-25T00:55:16.314582Z",
     "shell.execute_reply": "2024-01-25T00:55:16.313499Z"
    },
    "papermill": {
     "duration": 7.115264,
     "end_time": "2024-01-25T00:55:16.316593",
     "exception": false,
     "start_time": "2024-01-25T00:55:09.201329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       # Date          Time  Julian date  Local Siderial    Xcore    Ycore  0  \\\n",
      "0    20140624   53933.92820  2456832.736        4.263530  -6.3408  20.4095  0   \n",
      "1    20140705   73421.20612  2456843.816        4.955041  -7.1231  18.8881  0   \n",
      "2    20140723   52732.08209  2456861.727        4.709835  -7.4253  18.9204  0   \n",
      "3    20140829   74736.05817  2456898.825        5.959167  -8.6920  20.0007  0   \n",
      "4    20140829  110357.86420  2456898.961        0.535066  -7.3625  22.2284  0   \n",
      "..        ...           ...          ...             ...      ...      ... ..   \n",
      "669  20140626   73318.90217  2456834.815        4.795622  -6.1932  22.2920  0   \n",
      "670  20140626   74059.02815  2456834.820        4.829239  -7.9590  20.9098  0   \n",
      "671  20140626   74144.29622  2456834.821        4.832520  -7.9716  21.5741  0   \n",
      "672  20140626   73706.67222  2456834.817        4.812248 -10.0983  20.2481  0   \n",
      "673  20140626   74152.58822  2456834.821        4.833104  -7.7783  22.1255  0   \n",
      "\n",
      "       Theta       Phi  MD Rp    MD Psi  Energy (EeV)    Xmax        RA  \\\n",
      "0    58.2882  -92.1876  1.034   62.9991       0.01790  574.16  242.3146   \n",
      "1    36.8833  -41.9923  0.675  123.6145       0.01240  643.12  310.9607   \n",
      "2    49.0369  -45.1193  0.619  126.2902       0.02270  605.54  302.0516   \n",
      "3    50.6077  -34.5398  0.813   42.5562       0.00547  588.38   21.0673   \n",
      "4    60.3050 -104.6419  1.324   33.5186       0.01070  585.35   17.1719   \n",
      "..       ...       ...    ...       ...           ...     ...       ...   \n",
      "669  56.6442 -103.6872  1.547   34.5416       0.02150  490.19  262.8887   \n",
      "670  50.1334  -56.7599  0.848   40.9834       0.01610  565.70  301.6858   \n",
      "671  59.6761  -83.9464  0.983   32.3869       0.03240  638.19  282.4475   \n",
      "672  51.3240  -43.3142  1.958   47.9061       0.13100  654.66  310.3416   \n",
      "673  60.5406  -90.8238  1.203   31.2326       0.06010  572.74  276.1469   \n",
      "\n",
      "     Declination  Mir ID  Pass Code  Primary ID  Primary MC Weight  \n",
      "0       -18.9622    21.0        5.0         5.0             1.0000  \n",
      "1        11.2937    17.0        5.0         5.0             1.0395  \n",
      "2         0.0644    18.0        5.0         1.0             1.0000  \n",
      "3         3.6031    17.0        5.0         1.0             1.0947  \n",
      "4       -19.6740    23.0        5.0         2.0             1.0151  \n",
      "..           ...     ...        ...         ...                ...  \n",
      "669     -16.2486    23.0        5.0         3.0             1.0000  \n",
      "670      -5.2108    17.0        5.0         3.0             1.0904  \n",
      "671     -20.1517    21.0        5.0         3.0             1.0000  \n",
      "672      -1.0697    18.0        5.0         3.0             1.0000  \n",
      "673     -21.2394    21.0        5.0         3.0             1.0000  \n",
      "\n",
      "[674 rows x 19 columns]\n",
      "Training Set Shape: x_train - torch.Size([505, 12]) y_train - 505 Shape Check - True\n",
      "Testing Set Shape:  x_test - torch.Size([169, 12]) y_test - 169 Shape Check - True\n",
      "Species: [1. 2. 3. 4. 5.] , Length of NDarray 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s][2024-01-25 00:55:13,688] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2024-01-25 00:55:13,754] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2024-01-25 00:55:13,759] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compiler_fn\n",
      "[2024-01-25 00:55:13,962] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compiler_fn\n",
      "Epoch: 1, tloss 1.4950764179229736:   3%|▎         | 1/32 [00:01<00:32,  1.05s/it][2024-01-25 00:55:14,825] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2024-01-25 00:55:14,861] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2024-01-25 00:55:14,864] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compiler_fn\n",
      "[2024-01-25 00:55:15,023] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compiler_fn\n",
      "[2024-01-25 00:55:15,030] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2024-01-25 00:55:15,068] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2024-01-25 00:55:15,071] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function compiler_fn\n",
      "[2024-01-25 00:55:15,219] torch._dynamo.output_graph: [INFO] Step 2: done compiler function compiler_fn\n",
      "Epoch: 1, tloss: 1.360242247581482, vloss: 0.934318, : 100%|██████████| 32/32 [00:01<00:00, 20.69it/s]\n",
      "Epoch: 2, tloss: 0.6482977271080017, vloss: 0.895665, Improvement found, counter reset to 0: 100%|██████████| 32/32 [00:00<00:00, 242.77it/s]\n",
      "Epoch: 3, tloss: 0.7563130855560303, vloss: 0.855205, Improvement found, counter reset to 0: 100%|██████████| 32/32 [00:00<00:00, 336.67it/s]\n",
      "Epoch: 4, tloss: 0.7793055176734924, vloss: 0.855271, No improvement in the last 1 epochs: 100%|██████████| 32/32 [00:00<00:00, 339.68it/s]\n",
      "Epoch: 5, tloss: 0.49940362572669983, vloss: 0.844927, Improvement found, counter reset to 0: 100%|██████████| 32/32 [00:00<00:00, 322.38it/s]\n",
      "Epoch: 6, tloss: 1.0312690734863281, vloss: 0.843658, Improvement found, counter reset to 0: 100%|██████████| 32/32 [00:00<00:00, 366.62it/s]\n",
      "Epoch: 7, tloss: 0.2520065903663635, vloss: 0.870628, No improvement in the last 1 epochs: 100%|██████████| 32/32 [00:00<00:00, 343.93it/s]\n",
      "Epoch: 8, tloss: 0.6623995900154114, vloss: 0.878425, No improvement in the last 2 epochs: 100%|██████████| 32/32 [00:00<00:00, 337.65it/s]\n",
      "Epoch: 9, tloss: 0.547433614730835, vloss: 0.918214, No improvement in the last 3 epochs: 100%|██████████| 32/32 [00:00<00:00, 328.40it/s]\n",
      "Epoch: 10, tloss: 0.29640746116638184, vloss: 0.889447, No improvement in the last 4 epochs: 100%|██████████| 32/32 [00:00<00:00, 360.11it/s]\n",
      "Epoch: 11, tloss: 0.9296450018882751, vloss: 0.932581, Early stopping triggered after 5 epochs.: 100%|██████████| 32/32 [00:00<00:00, 346.79it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(69)\n",
    "torch.manual_seed(69)\n",
    "\n",
    "# Class from Jeff Heaton on early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_model = None\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.status = \"\"\n",
    "\n",
    "    def __call__(self, model, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "        elif self.best_loss - val_loss >= self.min_delta:\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            self.status = f\"Improvement found, counter reset to {self.counter}\"\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            self.status = f\"No improvement in the last {self.counter} epochs\"\n",
    "            if self.counter >= self.patience:\n",
    "                self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_model)\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "# Load the MC data and process it into tensors to be used by NN\n",
    "# modeled after Jeff Heaton's function\n",
    "def load_data():\n",
    "    # Read in data from Kaggle input directory\n",
    "    data = []\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            data.append(os.path.join(dirname, filename))\n",
    "            \n",
    "    # Read in the data into dataframes\n",
    "    df_list = []\n",
    "    for i in data:\n",
    "        df_list.append(pd.read_csv(i))\n",
    "    \n",
    "    # Remove last row of data due to incompete entry\n",
    "    for i,df in enumerate(df_list):\n",
    "        df_list[i] = df[:-1]\n",
    "\n",
    "    # Concat two MC df's into one df, ignore_index is important to make sure indexes append properly\n",
    "    full_data = pd.concat(df_list, ignore_index=True)\n",
    "    \n",
    "    # Reindex the df so it is randomized\n",
    "    # np.random.seed(42) # Uncomment this line to get the same shuffle each time\n",
    "    #full_data = full_data.reindex(np.random.permutation(full_data.index))\n",
    "    print(full_data)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    x = full_data[[\"Xcore\", \"Ycore\", \"Theta\", \"Phi\", \"MD Rp\", \"MD Psi\",\"Energy (EeV)\", \"Xmax\", \"RA\", \"Declination\", \"Mir ID\", \"Pass Code\"]].values\n",
    "    y = le.fit_transform(full_data[\"Primary ID\"])\n",
    "    #y = full_data[\"Primary ID\"].values\n",
    "    species = le.classes_\n",
    "    \n",
    "    # Split into validation and training sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # Numpy to Torch Tensor\n",
    "    x_train = torch.tensor(x_train, device=device, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train, device=device, dtype=torch.long)\n",
    "\n",
    "    x_test = torch.tensor(x_test, device=device, dtype=torch.float32)\n",
    "    y_test = torch.tensor(y_test, device=device, dtype=torch.long)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, species\n",
    "\n",
    "x_train, x_test, y_train, y_test, species = load_data()\n",
    "print(\"Training Set Shape:\", \"x_train -\", x_train.shape, \"y_train -\",y_train.shape[0], \"Shape Check -\", True if x_train.shape[0] == y_train.shape[0] else False)\n",
    "print(\"Testing Set Shape: \", \"x_test -\",x_test.shape, \"y_test -\",y_test.shape[0], \"Shape Check -\", True if x_test.shape[0] == y_test.shape[0] else False)\n",
    "print(\"Species:\", species, \", Length of NDarray\", len(species))\n",
    "\n",
    "# Create datasets\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "dataset_train = TensorDataset(x_train, y_train)\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "dataset_test = TensorDataset(x_test, y_test)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Create model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(x_train.shape[1], 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 25),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(25, len(species)),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n",
    "\n",
    "model = torch.compile(model,backend=\"aot_eager\").to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()  # cross entropy loss\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "es = EarlyStopping()\n",
    "\n",
    "epoch = 0\n",
    "done = False\n",
    "while epoch < 1000 and not done:\n",
    "    epoch += 1\n",
    "    steps = list(enumerate(dataloader_train))\n",
    "    pbar = tqdm.tqdm(steps)\n",
    "    model.train()\n",
    "    for i, (x_batch, y_batch) in pbar:\n",
    "        y_batch_pred = model(x_batch.to(device))\n",
    "        loss = loss_fn(y_batch_pred, y_batch.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "        if i == len(steps) - 1:\n",
    "            model.eval()\n",
    "            pred = model(x_test)\n",
    "            vloss = loss_fn(pred, y_test)\n",
    "            if es(model, vloss):\n",
    "                done = True\n",
    "            pbar.set_description(\n",
    "                f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:>7f}, {es.status}\"\n",
    "            )\n",
    "        else:\n",
    "            pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f058b8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T00:55:16.396872Z",
     "iopub.status.busy": "2024-01-25T00:55:16.396438Z",
     "iopub.status.idle": "2024-01-25T00:55:16.402362Z",
     "shell.execute_reply": "2024-01-25T00:55:16.401469Z"
    },
    "papermill": {
     "duration": 0.047924,
     "end_time": "2024-01-25T00:55:16.404313",
     "exception": false,
     "start_time": "2024-01-25T00:55:16.356389",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.8436582684516907\n"
     ]
    }
   ],
   "source": [
    "pred = model(x_test)\n",
    "vloss = loss_fn(pred, y_test)\n",
    "print(f\"Loss = {vloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b90393a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T00:55:16.480170Z",
     "iopub.status.busy": "2024-01-25T00:55:16.479905Z",
     "iopub.status.idle": "2024-01-25T00:55:16.515664Z",
     "shell.execute_reply": "2024-01-25T00:55:16.514506Z"
    },
    "papermill": {
     "duration": 0.075834,
     "end_time": "2024-01-25T00:55:16.517662",
     "exception": false,
     "start_time": "2024-01-25T00:55:16.441828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "_, predict_classes = torch.max(pred, 1)\n",
    "correct = accuracy_score(y_test.cpu(), predict_classes.cpu())\n",
    "print(f\"Accuracy: {correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78f0da9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T00:55:16.594332Z",
     "iopub.status.busy": "2024-01-25T00:55:16.594062Z",
     "iopub.status.idle": "2024-01-25T00:55:16.606027Z",
     "shell.execute_reply": "2024-01-25T00:55:16.604955Z"
    },
    "papermill": {
     "duration": 0.051904,
     "end_time": "2024-01-25T00:55:16.607858",
     "exception": false,
     "start_time": "2024-01-25T00:55:16.555954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: tensor([2, 1, 2, 2, 2, 0, 2, 3, 2, 2, 4, 2, 2, 1, 1, 1, 0, 1, 1, 2, 0, 0, 2, 1,\n",
      "        3, 1, 2, 2, 0, 2, 2, 0, 1, 2, 1, 1, 1, 0, 2, 1, 2, 3, 2, 0, 2, 1, 4, 2,\n",
      "        2, 2, 0, 2, 2, 2, 0, 2, 2, 1, 1, 2, 0, 1, 0, 2, 1, 2, 1, 3, 2, 0, 1, 3,\n",
      "        1, 2, 4, 4, 0, 1, 4, 0, 2, 1, 0, 2, 2, 2, 1, 3, 1, 2, 4, 0, 1, 4, 1, 2,\n",
      "        0, 1, 2, 0, 2, 1, 3, 1, 1, 1, 0, 2, 2, 2, 2, 1, 1, 2, 4, 2, 0, 3, 2, 2,\n",
      "        0, 4, 2, 3, 3, 2, 2, 1, 2, 1, 3, 4, 1, 2, 2, 1, 0, 3, 2, 2, 2, 0, 1, 2,\n",
      "        1, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1,\n",
      "        1], device='cuda:0')\n",
      "Expected: tensor([2, 3, 2, 2, 2, 4, 2, 3, 2, 2, 3, 2, 2, 1, 4, 1, 1, 4, 3, 2, 3, 0, 2, 4,\n",
      "        0, 4, 2, 2, 1, 2, 2, 0, 1, 2, 1, 4, 0, 0, 2, 1, 2, 4, 2, 0, 2, 0, 0, 2,\n",
      "        2, 2, 3, 2, 2, 2, 3, 2, 0, 0, 0, 2, 0, 4, 4, 2, 4, 2, 4, 3, 2, 3, 0, 0,\n",
      "        0, 2, 3, 1, 3, 1, 0, 4, 2, 4, 4, 2, 2, 2, 3, 4, 3, 2, 3, 3, 4, 4, 4, 2,\n",
      "        0, 1, 2, 0, 2, 0, 0, 4, 3, 4, 1, 2, 2, 2, 2, 3, 4, 2, 4, 2, 4, 3, 2, 2,\n",
      "        4, 1, 2, 3, 1, 2, 2, 4, 2, 1, 1, 4, 1, 2, 2, 4, 0, 4, 2, 2, 2, 3, 1, 2,\n",
      "        0, 2, 2, 2, 4, 2, 2, 2, 2, 2, 3, 0, 2, 2, 2, 1, 2, 2, 2, 2, 1, 2, 2, 3,\n",
      "        1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predictions: {predict_classes}\")\n",
    "print(f\"Expected: {y_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea18fda2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T00:55:16.687740Z",
     "iopub.status.busy": "2024-01-25T00:55:16.687453Z",
     "iopub.status.idle": "2024-01-25T00:55:16.694640Z",
     "shell.execute_reply": "2024-01-25T00:55:16.693362Z"
    },
    "papermill": {
     "duration": 0.048022,
     "end_time": "2024-01-25T00:55:16.696777",
     "exception": false,
     "start_time": "2024-01-25T00:55:16.648755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 2. 3. 3. 3. 1. 3. 4. 3. 3. 5. 3. 3. 2. 2. 2. 1. 2. 2. 3. 1. 1. 3. 2.\n",
      " 4. 2. 3. 3. 1. 3. 3. 1. 2. 3. 2. 2. 2. 1. 3. 2. 3. 4. 3. 1. 3. 2. 5. 3.\n",
      " 3. 3. 1. 3. 3. 3. 1. 3. 3. 2. 2. 3. 1. 2. 1. 3. 2. 3. 2. 4. 3. 1. 2. 4.\n",
      " 2. 3. 5. 5. 1. 2. 5. 1. 3. 2. 1. 3. 3. 3. 2. 4. 2. 3. 5. 1. 2. 5. 2. 3.\n",
      " 1. 2. 3. 1. 3. 2. 4. 2. 2. 2. 1. 3. 3. 3. 3. 2. 2. 3. 5. 3. 1. 4. 3. 3.\n",
      " 1. 5. 3. 4. 4. 3. 3. 2. 3. 2. 4. 5. 2. 3. 3. 2. 1. 4. 3. 3. 3. 1. 2. 3.\n",
      " 2. 3. 3. 3. 2. 3. 3. 3. 3. 3. 2. 1. 3. 3. 3. 2. 3. 3. 3. 3. 2. 3. 3. 2.\n",
      " 2.]\n"
     ]
    }
   ],
   "source": [
    "print(species[predict_classes.cpu().detach()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b48d832b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-25T00:55:16.827112Z",
     "iopub.status.busy": "2024-01-25T00:55:16.826723Z",
     "iopub.status.idle": "2024-01-25T00:55:17.078095Z",
     "shell.execute_reply": "2024-01-25T00:55:17.077156Z"
    },
    "papermill": {
     "duration": 0.293651,
     "end_time": "2024-01-25T00:55:17.080620",
     "exception": false,
     "start_time": "2024-01-25T00:55:16.786969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2060e-01, 4.6174e-01, 5.1190e-03, 1.3016e-01, 2.8237e-01],\n",
      "        [8.9354e-04, 1.9929e-05, 9.9646e-01, 1.2002e-03, 1.4311e-03],\n",
      "        [4.9734e-03, 9.1202e-05, 9.8874e-01, 3.1163e-03, 3.0755e-03],\n",
      "        [1.7235e-02, 3.7906e-03, 9.4033e-01, 1.5570e-02, 2.3072e-02],\n",
      "        [6.8592e-01, 1.3318e-02, 1.4276e-01, 4.7620e-02, 1.1038e-01],\n",
      "        [2.9537e-13, 4.6199e-21, 1.0000e+00, 1.0605e-14, 2.0863e-16],\n",
      "        [2.5526e-01, 1.1671e-01, 1.1300e-02, 4.1610e-01, 2.0063e-01],\n",
      "        [6.1601e-02, 4.6370e-03, 8.8633e-01, 3.0486e-02, 1.6942e-02],\n",
      "        [2.9202e-02, 1.1061e-04, 9.6540e-01, 2.7677e-03, 2.5184e-03]],\n",
      "       device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Sum Check:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.exp(pred[1:10]))\n",
    "print(\"Sum Check: \",sum(torch.exp(pred[1])).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6e66a",
   "metadata": {
    "papermill": {
     "duration": 0.039812,
     "end_time": "2024-01-25T00:55:17.158978",
     "exception": false,
     "start_time": "2024-01-25T00:55:17.119166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4348020,
     "sourceId": 7469305,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4348052,
     "sourceId": 7469345,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 43.392113,
   "end_time": "2024-01-25T00:55:18.822145",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-25T00:54:35.430032",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
